{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've got the following components working now:\n",
    " * **Speaker database**. Looking at our dataset of audio speaker samples and returning anchor, positive, negative triplets\n",
    " * **Audio preprocessing**. Running FFT's on audio samples and breaking them into Mel filter bank energies (log frequency bands)\n",
    " * **Batch building**. Build a batch of 32 triplets (anchor, positive, negative), slice each sample into 4 second segments. Preprocess the audio and build a batch of correctly shaped tensors ready to feed into the model.\n",
    " * **Model**. A deep neural net model that takes preprocessed audio and produces **speaker embeddings**\n",
    " * **Loss function**.  A loss function used to optimize the model using [Triplet Loss](https://en.wikipedia.org/wiki/Triplet_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Processing speed\n",
    "I'm training about one batch per 24 seconds.  This breaks down into two major components:\n",
    "* **Batch building**. *9 sec*. Reading the audio files and running FFT transforms.  This could be done on another thread. Estimated speedup: about 1/3 faster.\n",
    "* **Set up GPU**. *15 sec*. I'm currently training on a CPU because I haven't got TensorFlow working on my GPU yet. Estimated speedup: at least 10x.\n",
    "\n",
    "Based on results I've seen from others doing similar things, I estimate I'll need about 100,000 batches to get good loss.  Just for laughs, let's look at what that'd take with the current approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an image from [Wallecplise](https://github.com/Walleclipse/Deep_Speaker-speaker_recognition_system) from their research doing speaker embedding training:\n",
    "![Walleclipse training losses over time](images/walleclipse-loss.png \"Walleclipse training losses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some funny/interesting things in there:\n",
    " * How did they have so many losses close to zero? Why did they still have so many losses at 4+ after 85k iterations? As of 329 iterations, I'm not seeing anything close to zero.\n",
    " * Why did they see so much variance? I'm not seeing much variance.  Right now I'm at 2.7-3.8 and not much outside of that.  I did see one batch at 1.25 loss at batch 276, but haven't seen it that low since."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.462962962962962"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_build_sec = 9\n",
    "training_sec = 15\n",
    "batch_train_sec = 22\n",
    "need_batches = 100000\n",
    "seconds_needed = batch_train_sec * need_batches\n",
    "hours_needed = seconds_needed / 3600\n",
    "days_needed = hours_needed / 24\n",
    "days_needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training situation may not be as bad as I thought.  After 350 epochs I was getting training losses from 2.1-4.2.  It started with losses around 5-6.5.  This was after 2-3 hours of CPU-only training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backlog\n",
    "Here's a prioritized backlog of things I need to do:\n",
    " * **Add timing** Show time needed to train a batch.  Break that time down into preprocessing and training.  You'll need this for determining when it's time to stop optimizing and just let the thing learn.\n",
    " * **Inference**. Make something that does inference.  Given two samples, decide if they're the same person.  Report on what the alpha is between anchor and positive, and anchor and negative.\n",
    " * **Configure GPU**\n",
    "     * After configuring GPU, estimate training time needed for 100,000 batches\n",
    " * **Chart loss** Show a continuously updated graph of the training process so I can verify that it's making progress.\n",
    " * **Optimize audio preprocessing**.  Either do this on another thread, or preprocess the entire dataset\n",
    " * **Document model architecture**\n",
    " * **Document training processing pipeline**\n",
    " \n",
    " ## Backlog: Done\n",
    " * **Checkpoint / reload models** Save the model as it's training periodically, and create a facility to resume training.  Determine how big the model file is, and determine if you want to save N number of models, all models, or just the last x_days worth of models, how often to save, etc.\n",
    "   * Models are 20Mb.\n",
    " * **Log losses**. Append to a log that shows training loss per iteration.  You'll need this for doing charting that persists between incremental training sessions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    " * [FaceNet paper](https://arxiv.org/pdf/1503.03832.pdf) by Florian Schroff, Dmitry Kalenichenko, James Philbin (Google)\n",
    " * [Deep Speaker paper](https://arxiv.org/pdf/1705.02304.pdf) Chao Li, Xiaokong Ma, Bing Jiang, Xiangang Li (Baidu)\n",
    " * [Walleclipse/Deep_Speaker](https://github.com/Walleclipse/Deep_Speaker-speaker_recognition_system)\n",
    " * [philipperemy/deep-speaker](https://github.com/philipperemy/deep-speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path\n",
    "os.path.exists(r'C:\\Users\\Richard Lack\\Documents\\notebooks\\voice-embeddings\\checkpoints\\x.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
