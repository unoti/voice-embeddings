{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype Recognizer\n",
    "In this notebook we'll be putting together something that listens to the audio and tries to tell who is talking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import audiostream\n",
    "import audiolib\n",
    "import application\n",
    "import config\n",
    "import models\n",
    "import numpy as np\n",
    "\n",
    "application.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming at batch 29731\n",
      "Loading model from checkpoints\\voice-embeddings.h5\n",
      "Preloaded model from checkpoints\\voice-embeddings.h5\n"
     ]
    }
   ],
   "source": [
    "model = application.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "4 seconds remaining\n",
      "3 seconds remaining\n",
      "2 seconds remaining\n",
      "1 seconds remaining\n",
      "Recording finished\n"
     ]
    }
   ],
   "source": [
    "clip1 = audiostream.record(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "4 seconds remaining\n",
      "3 seconds remaining\n",
      "2 seconds remaining\n",
      "1 seconds remaining\n",
      "Recording finished\n"
     ]
    }
   ],
   "source": [
    "clip2 = audiostream.record(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long are these sound samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clip1) / 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_embedding(model, sound):\n",
    "    \"\"\"\n",
    "    sound: a numpy array of 16-bit signed sound samples.\n",
    "    \"\"\"\n",
    "    print('extract soundlen=',len(sound))\n",
    "    features = audiolib.extract_features(sound, sample_rate=44100, num_filters=config.NUM_FILTERS)\n",
    "    emb = models.get_embedding(model, features)\n",
    "    return emb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.49375"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "399/160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to what's going wrong here is ```minibatch._clipped_audio``` and ```config.NUM_FRAMES```.  I don't quite understand the details yet (actually I figured it out; read on).  I thought that 160 frames would be 4 seconds, but I suspect that somehow it is not.\n",
    "\n",
    "Under the covers we're using [python_speech_features.fbank()](https://github.com/jameslyons/python_speech_features) with the default winstep parameter of 0.010 = 10ms.  At 160 frames, my theory is that we're actually processing not 4 seconds, but 1.6 seconds.  In particular, a randomly selected 1.6 seconds from the sample.\n",
    "\n",
    "This theory is consistent with the fact that I was getting exactly 2.5x the amount of time I expected.  This makes sense, because winlen=0.025 and winstep=0.010.  That's exactly a ratio of 2.5:1.\n",
    "\n",
    "I can determine what I want to do about this long term, but for now I think if I set up my sound buffer to have 1.6 seconds of sound this will work well enough to see if it generally works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sound_chunks(sound, chunk_seconds=1.6, step_seconds=0.5, sample_rate=44100):\n",
    "    \"\"\"Return a sequence of sound chunks from a sound clip.\n",
    "    Each chunk will be 1.6 seconds of the sound, and each\n",
    "    successive chunk will be advanced by the specified number of seconds.\n",
    "    sound: a numpy array of 16-bit signed integers representing a sound sample.\n",
    "    \"\"\"\n",
    "    chunk_len = int(chunk_seconds * sample_rate)\n",
    "    chunk_step = int(step_seconds * sample_rate)\n",
    "    chunk_count = int(len(sound) / chunk_step)\n",
    "    for i in range(chunk_count):\n",
    "        start = i * chunk_step\n",
    "        end = start + chunk_len\n",
    "        yield sound[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(sound_chunks(clip1, chunk_seconds=1.61))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.61"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[0]) / 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_from_sound(model, sound, sample_rate=44100):\n",
    "    \"\"\"Return a sequence of embeddings from the different time slices\n",
    "    in the sound clip.\n",
    "    sound: a numpy array of 16-bit signed integers representing a sound sample.\n",
    "    \"\"\"\n",
    "    # The 1.601 is a hack to make sure we end up with a shape of 160 instead of 159.\n",
    "    # What we actually want is 1.6.\n",
    "    #*TODO: Figure out a better way to fix the 159->160 off by one error than adding .001.\n",
    "    chunk_seconds=1.61\n",
    "    for chunk in sound_chunks(sound, chunk_seconds=chunk_seconds, sample_rate=sample_rate):\n",
    "        # The last portion of the sound may be less than our desired length.\n",
    "        # We can safely skip it because we'll process it later as it shifts down the time window.\n",
    "        lc = len(chunk)\n",
    "        print('lc=%d sec=%f delta=%f' % (lc, lc/sample_rate, lc/sample_rate - chunk_seconds))\n",
    "        if len(chunk)/sample_rate - chunk_seconds < -0.1:\n",
    "            continue\n",
    "        print('calculating embedding for chunk len=%d' % len(chunk))\n",
    "        yield calc_embedding(model, chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract soundlen= 71001\n",
      "extract soundlen= 71001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract soundlen= 71001\n",
      "extract soundlen= 71001\n",
      "extract soundlen= 71001\n"
     ]
    }
   ],
   "source": [
    "embs1 = list(embeddings_from_sound(model, clip1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.61"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "71001/44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame Length and FFT size\n",
    "I've been having this problem while processing sound samples into filter banks:\n",
    "> WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
    "\n",
    "This problem is discussed in this [github issue on Python Speech Features](https://github.com/jameslyons/python_speech_features).\n",
    "\n",
    "The recommendation there is to read this [Practical Cryptography tutorial on Mel Frequency Cepstral Coefficients mfccs](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)\n",
    "\n",
    "The recommendation is:\n",
    "> Please read this [tutorial](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/) to better understand the relationship between FFT size and frame length. The FFT size should be longer than the frame length. either increase your fft size, or decrease your frame length, or you can ignore the warning and see how it performs.\n",
    "\n",
    "Investigating this further, I found this [pull request](https://github.com/jameslyons/python_speech_features/pull/76/commits/9ab32879b1fb31a38c1a70392fd21370b8fdc30f) that makes NFFT a power of two and simply increases it until it fits.  The comments in the PR say:\n",
    "> Having an FFT less than the window length loses precision by dropping\n",
    "    many of the samples; a longer FFT than the window allows zero-padding\n",
    "    of the FFT buffer which is neutral in terms of frequency domain conversion.\n",
    "\n",
    "So it sounds like I could increase the numfft parameter on ```fbank()``` from its default of 512 to 2048.  But why am I getting this warning in the first place?  Am I giving it the wrong sample rate?  I didn't have this problem when running the training data through this code.  So I need to figure out how to run my real-time captured sound data through the same code without it breaking.  I want to use the same processing code for training and inference, so I'm not liking the idea of changing the ```numfft``` parameter in ```fbank()```.\n",
    "\n",
    "Let's compare some training data to my realtime captured audio and try to zero in on what's going wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Realtime Audio to Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176400"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clip1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clip1)/44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = application.make_speaker_db()\n",
    "triplet = db.random_triplet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('d:\\\\datasets\\\\voxceleb1\\\\vox1\\\\wav\\\\id10716\\\\yKO2BD79hQ0\\\\00012.wav',\n",
       " 'd:\\\\datasets\\\\voxceleb1\\\\vox1\\\\wav\\\\id10716\\\\Pvm_Dv1P3-M\\\\00001.wav',\n",
       " 'd:\\\\datasets\\\\voxceleb1\\\\vox1\\\\wav\\\\id10161\\\\6KdOSVcTQNc\\\\00001.wav',\n",
       " 'id10716',\n",
       " 'id10161')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, sample = audiolib.load_wav(triplet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an awfully low sample rate, isn't it? Is that actually correct?  I would expect 44100 not 16000.  Let's see if these numbers check out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.6400625"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)/rate # Expected number of seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I opened this sound clip up in Audacity and verified that it is in fact 9.64 seconds long.That could explain why I'm getting such a big difference in working with my own sound clips.  My sound clips are sampled at a significantly higher rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = audiolib.extract_features(sample, sample_rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there was no warning.  I've created a resampled version of this same clip, resampled from a rate of 16000 to 44100.  Let's see how that fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate2, sample2 = audiolib.load_wav(r'd:\\tmp\\00012-441.wav')\n",
    "rate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.640068027210884"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample2)/rate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    }
   ],
   "source": [
    "features2 = audiolib.extract_features(sample2, sample_rate=rate2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! So the same cound clip, upsampled from 16000 to 44100 gives me this warning.\n",
    "\n",
    "I either need to pre-downsample my sound clips or up the number of FFT's in the feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "Here are a couple of options for downsampling that I found in this [thread](https://stackoverflow.com/questions/30619740/downsampling-wav-audio-file):\n",
    " * [audioop](https://docs.python.org/2/library/audioop.html) ```ratecv``` In the python standard library.\n",
    " * [scipi signal resample](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.resample.html#scipy.signal.resample)\n",
    " \n",
    " I'm going to try the standard library approach.  To verify that I've got it downsampling properly I want to save out a wav file.  I won't need to save wav files for the actual application, but I don't want to blindly use the sound data I've downsampled; I want to verify it's really doing what I think it is.\n",
    "\n",
    "From [audioop Docs](https://docs.python.org/2/library/audioop.html):\n",
    "```audioop.ratecv(fragment, width, nchannels, inrate, outrate, state[, weightA[, weightB]])```\n",
    " > state is a tuple containing the state of the converter. The converter returns a tuple (newfragment, newstate), and newstate should be passed to the next call of ratecv(). The initial call should pass None as the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176400"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clip1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import audioop\n",
    "def downsample(samples, from_rate=44100, to_rate=16000):\n",
    "    width = 2\n",
    "    nchannels = 1\n",
    "    state = None\n",
    "    fragment, new_state = audioop.ratecv(samples, width, nchannels, from_rate, to_rate, state)\n",
    "    return fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip1_d = downsample(clip1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176400, 128000, 4.0, 8.0)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clip1), len(clip1_d), len(clip1)/44100, len(clip1_d)/16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128000 / 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is that each entry in my samples is 16 bits, and this thing is expecting a byte-like object.  I've through of 3 options:\n",
    " 1. Pass ```ratecv``` width=1\n",
    " 2. Do the conversion inside of AudioStream\n",
    " 3. Tell AudioStream that I want samples at 16000 instead of 44100.\n",
    " \n",
    "Let's try option 3 first, because if that works, it will be cleaner and eliminate needless conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "6 seconds remaining\n",
      "5 seconds remaining\n",
      "4 seconds remaining\n",
      "3 seconds remaining\n",
      "2 seconds remaining\n",
      "1 seconds remaining\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "as2 = audiostream.AudioStream(seconds=4, rate=16000)\n",
    "as2.start()\n",
    "seconds=6\n",
    "print('Recording')\n",
    "for i in range(seconds):\n",
    "    print('%d seconds remaining' % (seconds - i))\n",
    "    time.sleep(1)\n",
    "as2.stop()\n",
    "clip3 = as2.sound_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clip3) / 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "features3 = audiolib.extract_features(clip3, sample_rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok that worked with no warnings.  Let's try saving that to a wav file to make sure it's actually producing sound the way we expect and not just crazy data.\n",
    "\n",
    "We'll use the Python standard library [Wave module](https://docs.python.org/2/library/wave.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "def save_wav(filename, samples, rate=16000, width=2, channels=1):\n",
    "    wav = wave.open(filename, 'wb')\n",
    "    wav.setnchannels(channels)\n",
    "    wav.setsampwidth(width)\n",
    "    wav.setframerate(rate)\n",
    "    wav.writeframes(samples)\n",
    "    wav.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_wav(r'd:\\tmp\\clip3.wav', clip3, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does produce a wav file as I expect, so this seems to be working from the realtime audio streaming all the way to producing sound at the desired sampling rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with some clips\n",
    "Now that we've got the issues with the sampling rate all sorted out (the issue is I need to use a rate of 16000 not 44100), let's try this with some live recorded clips.  We'll do that in the next notebook, \"6-Prototype Speaker Recognizer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "4 seconds remaining\n",
      "3 seconds remaining\n",
      "2 seconds remaining\n",
      "1 seconds remaining\n",
      "Recording finished\n"
     ]
    }
   ],
   "source": [
    "clip1 = audiostream.record(4, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "4 seconds remaining\n",
      "3 seconds remaining\n",
      "2 seconds remaining\n",
      "1 seconds remaining\n",
      "Recording finished\n"
     ]
    }
   ],
   "source": [
    "clip2 = audiostream.record(4, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiolib.save_wav(r'd:\\tmp\\clip1.wav', clip1, rate=16000)\n",
    "audiolib.save_wav(r'd:\\tmp\\clip2.wav', clip2, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lc=25760 sec=1.610000 delta=0.000000\n",
      "calculating embedding for chunk len=25760\n",
      "extract soundlen= 25760\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (160, 64, 1) but got array with shape (57, 64, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-dda21957c11b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membs1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_from_sound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-283d5f252e12>\u001b[0m in \u001b[0;36membeddings_from_sound\u001b[1;34m(model, sound, sample_rate)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'calculating embedding for chunk len=%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32myield\u001b[0m \u001b[0mcalc_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-cd58d7ffaf2b>\u001b[0m in \u001b[0;36mcalc_embedding\u001b[1;34m(model, sound)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'extract soundlen='\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudiolib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m44100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_FILTERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\notebooks\\voice-embeddings\\models.py\u001b[0m in \u001b[0;36mget_embedding\u001b[1;34m(model, sample)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \"\"\"\n\u001b[0;32m    138\u001b[0m     \u001b[0minput_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# .predict() wants a batch, not a single entry.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m     \u001b[0memb_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Predict for this batch of 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m     \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_rows\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Predict() returns a batch of results. Extract the single row.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\prg\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\prg\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\prg\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (160, 64, 1) but got array with shape (57, 64, 1)"
     ]
    }
   ],
   "source": [
    "embs1 = list(embeddings_from_sound(model, clip1, sample_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8ef3af330e99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membs1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "len(embs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
